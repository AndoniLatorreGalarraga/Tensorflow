{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
     ]
    }
   ],
   "source": [
    "vocab = dict()\n",
    "word_encoding = 1\n",
    "def bag_of_words(text):\n",
    "    global word_encoding\n",
    "\n",
    "    words = text.lower().split(' ')\n",
    "    bag = dict()\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            encoding = vocab[word]\n",
    "        else:\n",
    "            vocab[word] = word_encoding\n",
    "            encoding = word_encoding\n",
    "            word_encoding += 1\n",
    "        \n",
    "        bag.setdefault(encoding, 0)\n",
    "        bag[encoding] += 1\n",
    "        \n",
    "    return bag\n",
    "\n",
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "bag = bag_of_words(text)\n",
    "print(bag)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words doesn't encode the order of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1}\n",
      "Negative: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 21: 1, 18: 1, 19: 1, 20: 1, 17: 1}\n"
     ]
    }
   ],
   "source": [
    "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
    "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
    "\n",
    "pos_bag = bag_of_words(positive_review)\n",
    "neg_bag = bag_of_words(negative_review)\n",
    "\n",
    "print(\"Positive:\", pos_bag)\n",
    "print(\"Negative:\", neg_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Integer encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n",
      "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
     ]
    }
   ],
   "source": [
    "vocab = dict()\n",
    "word_encoding = 1\n",
    "def one_hot_encoding(text):\n",
    "    global word_encoding\n",
    "\n",
    "    words = text.lower().split(' ')\n",
    "    encoding = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            code = vocab[word]\n",
    "            encoding.append(code)\n",
    "        else:\n",
    "            vocab[word] = word_encoding\n",
    "            encoding.append(word_encoding)\n",
    "            word_encoding += 1\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "encoding = one_hot_encoding(text)\n",
    "print(encoding)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: [10, 11, 12, 13, 14, 15, 5, 16, 17, 18, 19, 14, 20, 21]\n",
      "Negative: [10, 11, 12, 13, 14, 15, 5, 16, 21, 18, 19, 14, 20, 17]\n"
     ]
    }
   ],
   "source": [
    "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
    "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
    "\n",
    "pos_encode = one_hot_encoding(positive_review)\n",
    "neg_encode = one_hot_encoding(negative_review)\n",
    "\n",
    "print(\"Positive:\", pos_encode)\n",
    "print(\"Negative:\", neg_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word enbedings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word -> vector\n",
    "It's a layer, pretrined models exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- simple RNN layer\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "VOCAB_SIZE = 88584\n",
    "\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     1,    14,    22,    16,\n",
       "          43,   530,   973,  1622,  1385,    65,   458,  4468,    66,\n",
       "        3941,     4,   173,    36,   256,     5,    25,   100,    43,\n",
       "         838,   112,    50,   670, 22665,     9,    35,   480,   284,\n",
       "           5,   150,     4,   172,   112,   167, 21631,   336,   385,\n",
       "          39,     4,   172,  4536,  1111,    17,   546,    38,    13,\n",
       "         447,     4,   192,    50,    16,     6,   147,  2025,    19,\n",
       "          14,    22,     4,  1920,  4613,   469,     4,    22,    71,\n",
       "          87,    12,    16,    43,   530,    38,    76,    15,    13,\n",
       "        1247,     4,    22,    17,   515,    17,    12,    16,   626,\n",
       "          18, 19193,     5,    62,   386,    12,     8,   316,     8,\n",
       "         106,     5,     4,  2223,  5244,    16,   480,    66,  3785,\n",
       "          33,     4,   130,    12,    16,    38,   619,     5,    25,\n",
       "         124,    51,    36,   135,    48,    25,  1415,    33,     6,\n",
       "          22,    12,   215,    28,    77,    52,     5,    14,   407,\n",
       "          16,    82, 10311,     8,     4,   107,   117,  5952,    15,\n",
       "         256,     4, 31050,     7,  3766,     5,   723,    36,    71,\n",
       "          43,   530,   476,    26,   400,   317,    46,     7,     4,\n",
       "       12118,  1029,    13,   104,    88,     4,   381,    15,   297,\n",
       "          98,    32,  2071,    56,    26,   141,     6,   194,  7486,\n",
       "          18,     4,   226,    22,    21,   134,   476,    26,   480,\n",
       "           5,   144,    30,  5535,    18,    51,    36,    28,   224,\n",
       "          92,    25,   104,     4,   226,    65,    16,    38,  1334,\n",
       "          88,    12,    16,   283,     5,    16,  4472,   113,   103,\n",
       "          32,    15,    16,  5345,    19,   178,    32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 45s 69ms/step - loss: 0.4365 - acc: 0.8000 - val_loss: 0.3114 - val_acc: 0.8676\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 43s 69ms/step - loss: 0.2434 - acc: 0.9070 - val_loss: 0.2761 - val_acc: 0.8890\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 39s 63ms/step - loss: 0.1878 - acc: 0.9316 - val_loss: 0.3399 - val_acc: 0.8654\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3572 - acc: 0.8623\n",
      "[0.35722780227661133, 0.8623200058937073]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "      if num != PAD:\n",
    "        text += reverse_word_index[num] + \" \"\n",
    "\n",
    "    return text[:-1]\n",
    "  \n",
    "print(decode_integers(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81147456]\n",
      "[0.28104258]\n"
     ]
    }
   ],
   "source": [
    "def predict(text):\n",
    "  encoded_text = encode_text(text)\n",
    "  pred = np.zeros((1,250))\n",
    "  pred[0] = encoded_text\n",
    "  result = model.predict(pred) \n",
    "  print(result[0])\n",
    "\n",
    "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
    "predict(positive_review)\n",
    "\n",
    "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(negative_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN play generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading your own data is a possibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the contents of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "  return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "def int_to_text(ints):\n",
    "  try:\n",
    "    ints = ints.numpy()\n",
    "  except:\n",
    "    pass\n",
    "  return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(2):\n",
    "  print(\"\\n\\nEXAMPLE\\n\")\n",
    "  print(\"INPUT\")\n",
    "  print(int_to_text(x))\n",
    "  print(\"\\nOUTPUT\")\n",
    "  print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[-3.2729932e-04  3.1751557e-03 -5.9531932e-03 ... -1.2554172e-03\n",
      "    1.8108833e-04 -6.3380776e-03]\n",
      "  [ 2.1914975e-03 -1.4303051e-03 -3.8781504e-03 ... -4.2909747e-03\n",
      "   -1.8989781e-03  2.0868718e-03]\n",
      "  [ 1.5592678e-03 -7.0976710e-04 -6.2429840e-03 ...  4.3531740e-04\n",
      "   -2.0877537e-03  7.5717107e-05]\n",
      "  ...\n",
      "  [ 1.5653133e-03  3.4541423e-03  9.0903407e-03 ... -2.0071308e-03\n",
      "    7.2882895e-04  1.0070583e-03]\n",
      "  [ 1.5111169e-03  4.2188480e-03  5.4279929e-03 ... -3.2007727e-03\n",
      "    2.3605190e-03 -2.1470897e-03]\n",
      "  [ 8.4742554e-05  3.7038261e-03  2.9059593e-03 ... -2.3623868e-03\n",
      "   -5.3060410e-04  1.9151808e-04]]\n",
      "\n",
      " [[-4.1277669e-03  2.9524842e-03 -5.8646305e-03 ... -2.6433694e-04\n",
      "   -5.4154010e-03 -5.5851704e-03]\n",
      "  [-1.6012628e-02  2.0003407e-03 -7.0246425e-03 ...  1.2613363e-03\n",
      "   -1.2339876e-02 -5.7949251e-03]\n",
      "  [-1.5033163e-02  9.5824944e-05 -9.9991895e-03 ...  7.1492803e-04\n",
      "   -8.1328349e-03 -4.1670986e-03]\n",
      "  ...\n",
      "  [ 9.1039389e-03  4.9270736e-03 -5.5722287e-03 ... -8.9185238e-03\n",
      "    2.0543397e-03 -3.5865468e-03]\n",
      "  [ 1.4119558e-02  3.7194488e-03 -2.3917984e-03 ... -7.5103492e-03\n",
      "    5.5025006e-04  6.6769193e-04]\n",
      "  [ 1.1011207e-02  6.3335481e-03 -7.5414712e-03 ... -6.4937314e-03\n",
      "    1.2539653e-03 -7.0575709e-03]]\n",
      "\n",
      " [[-3.2729932e-04  3.1751557e-03 -5.9531932e-03 ... -1.2554172e-03\n",
      "    1.8108833e-04 -6.3380776e-03]\n",
      "  [-1.2625366e-02  2.3386739e-03 -6.7697451e-03 ... -3.8431282e-04\n",
      "   -9.1558751e-03 -5.8606556e-03]\n",
      "  [-5.0582029e-03 -5.4468052e-04 -6.7754239e-03 ...  1.2518234e-03\n",
      "   -7.6102670e-03 -2.9340263e-03]\n",
      "  ...\n",
      "  [ 5.9068603e-03  6.9499812e-03  4.5305262e-03 ... -4.5753126e-03\n",
      "   -3.8472319e-03 -2.8904895e-03]\n",
      "  [ 4.8991898e-03  6.8754000e-03  5.3319894e-04 ...  7.2303100e-04\n",
      "   -3.9868676e-03 -4.2995671e-03]\n",
      "  [ 1.2044197e-02  3.0672816e-03 -1.1610760e-03 ...  2.4614471e-04\n",
      "   -2.3385137e-04 -3.0065132e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.5688307e-03  3.2902802e-03  8.7768078e-04 ... -4.2769667e-03\n",
      "   -1.1733858e-03 -1.2234012e-03]\n",
      "  [-4.0826793e-03  1.7531151e-03 -4.0540914e-03 ... -2.6506647e-03\n",
      "    5.5493088e-05 -7.5383193e-04]\n",
      "  [-3.0517958e-03  5.7683280e-03 -8.5837040e-03 ... -2.5668824e-03\n",
      "    8.0927578e-04 -6.1967676e-03]\n",
      "  ...\n",
      "  [-6.4096088e-03 -4.9408507e-03  7.5925875e-04 ... -6.2915669e-03\n",
      "    7.1403747e-03 -8.0429297e-03]\n",
      "  [-1.8474048e-02 -4.6727085e-03 -1.9569427e-03 ... -3.3056187e-03\n",
      "   -4.0599424e-03 -7.5732232e-03]\n",
      "  [-1.4392043e-02 -8.5834507e-04 -7.2977571e-03 ... -3.7817785e-03\n",
      "   -3.7868165e-03 -1.1690210e-02]]\n",
      "\n",
      " [[ 3.7970245e-03 -2.3845867e-03 -1.8387842e-03 ...  2.1344626e-03\n",
      "   -8.9005334e-05  8.6763530e-04]\n",
      "  [-5.4602455e-03 -5.0807237e-03 -2.7490931e-03 ... -8.8230823e-04\n",
      "    1.4306477e-03 -3.7242554e-04]\n",
      "  [-4.7142711e-03 -1.0274462e-03 -7.7894842e-03 ... -2.2461123e-03\n",
      "    1.0148652e-03 -6.0780360e-03]\n",
      "  ...\n",
      "  [ 1.3207679e-02  6.9924723e-03 -1.6225327e-04 ... -9.4145490e-03\n",
      "    2.7988295e-04 -4.2357198e-03]\n",
      "  [ 1.0670752e-02  6.9425879e-03 -4.4162013e-03 ... -4.0889638e-03\n",
      "   -2.3334508e-03 -3.3809708e-03]\n",
      "  [ 1.2681292e-02  3.3520965e-03 -6.3431454e-03 ... -1.2306968e-03\n",
      "   -1.5909259e-03 -6.4541644e-04]]\n",
      "\n",
      " [[-3.2729932e-04  3.1751557e-03 -5.9531932e-03 ... -1.2554172e-03\n",
      "    1.8108833e-04 -6.3380776e-03]\n",
      "  [-1.3388233e-03  6.1387611e-03 -3.6233328e-03 ... -5.5434778e-03\n",
      "   -1.3143350e-03 -5.7740444e-03]\n",
      "  [-7.7741698e-04  1.1123698e-02 -1.1121184e-03 ... -1.0333498e-02\n",
      "   -4.1019972e-03 -5.7410361e-04]\n",
      "  ...\n",
      "  [-4.4448543e-03  5.8287242e-03 -1.8290836e-02 ... -5.9484774e-03\n",
      "   -9.0826256e-03 -1.4179695e-02]\n",
      "  [-4.7830422e-03  8.1994655e-03 -1.3782499e-02 ... -8.5308328e-03\n",
      "   -7.8769894e-03 -1.1999922e-02]\n",
      "  [-2.1551575e-03  8.8329148e-03 -1.0583454e-02 ... -3.6633709e-03\n",
      "   -7.3029809e-03 -1.6767165e-02]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[-3.2729932e-04  3.1751557e-03 -5.9531932e-03 ... -1.2554172e-03\n",
      "   1.8108833e-04 -6.3380776e-03]\n",
      " [ 2.1914975e-03 -1.4303051e-03 -3.8781504e-03 ... -4.2909747e-03\n",
      "  -1.8989781e-03  2.0868718e-03]\n",
      " [ 1.5592678e-03 -7.0976710e-04 -6.2429840e-03 ...  4.3531740e-04\n",
      "  -2.0877537e-03  7.5717107e-05]\n",
      " ...\n",
      " [ 1.5653133e-03  3.4541423e-03  9.0903407e-03 ... -2.0071308e-03\n",
      "   7.2882895e-04  1.0070583e-03]\n",
      " [ 1.5111169e-03  4.2188480e-03  5.4279929e-03 ... -3.2007727e-03\n",
      "   2.3605190e-03 -2.1470897e-03]\n",
      " [ 8.4742554e-05  3.7038261e-03  2.9059593e-03 ... -2.3623868e-03\n",
      "  -5.3060410e-04  1.9151808e-04]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred) # notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[-0.0003273   0.00317516 -0.00595319  0.00124646  0.00307925  0.00133406\n",
      " -0.00358953 -0.00159095  0.00034507  0.00336984 -0.0025455  -0.00380446\n",
      "  0.00356946  0.00287322  0.00102077 -0.00145042  0.00325365  0.00844909\n",
      " -0.00371635 -0.00042692 -0.0031486  -0.0045115   0.00071089  0.00400226\n",
      "  0.00641349 -0.00235978 -0.00122275  0.00155455  0.00073546 -0.00080741\n",
      " -0.004888   -0.00663896  0.00125374 -0.00385286  0.00483243 -0.00159594\n",
      "  0.0009032  -0.00142138 -0.00044307 -0.0013528  -0.0053758   0.00347007\n",
      " -0.00421145 -0.00187597 -0.00158415  0.00551064  0.00290984  0.0013058\n",
      "  0.00152392  0.00107584  0.00633888  0.00520467 -0.00531372  0.00354684\n",
      " -0.00378028  0.00167322  0.00182533 -0.00495441 -0.01033763 -0.00143316\n",
      " -0.0061624  -0.0042495  -0.00125542  0.00018109 -0.00633808], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred) # 65 values representing the probabillity of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DcnqjoKz$LffXhRTvp!SMr,UyUG Al:WjlWxGJDE!vIU-'LtW$oySTZeqGz X.TDp!qy?RMBpszy3arz3oQcry$,CJL:yTlE'J,g\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars  # and this is what the model predicted for training sequence 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = 'D:/.temp/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load any checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_num = 10\n",
    "# model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
    "# model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 800\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "    \n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Come to me, for I had then last\n",
      "with mine own folly--\n",
      "And hear the sequel of your arms in loving to a happy evil:\n",
      "Hich I have infect his realm as now Marcius dignift's reband?\n",
      "\n",
      "MONTAGUE:\n",
      "Many a minutes could\n",
      "not back again to enter.\n",
      "\n",
      "MISTRESS OVERDONE:\n",
      "Well, well; therefore plantagenet mistrust\n",
      "A trial of our slavish richard by the nose,\n",
      "They be true heels, like unfolds all men ll GRICHARD III:\n",
      "Faith, serve me, sonisted how to marry other.\n",
      "Go men our noble ancestry athere's himself\n",
      "Against both, which 'longs embraces him.\n",
      "\n",
      "First Servingman:\n",
      "Why learns that way, my lord?\n",
      "\n",
      "PRINCE:\n",
      "Sir, these abject\n",
      "Shall will either perfection and their betterness, I\n",
      "ngied to repent our majesty.\n",
      "\n",
      "ADLAND:\n",
      "Yes, I know him; 'tis a man of Pisa; by rinners love;\n",
      "And I beseech your majesty to come again.\n",
      "\n",
      "ROMEO:\n",
      "I do beseec\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
